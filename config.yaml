# Training configuration for train_qLora.py
# You can edit this file to change defaults without passing CLI flags.

base_model: meta-llama/Llama-3.1-8B
tiny_debug_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0

lora:
  r: 8
  alpha: 32
  dropout: 0.05

learning_rate: 0.0002
batch_size: 4
grad_accum: 16
max_seq_len: 512
epochs: 3
save_dir: ./lora_out
seed: 42

# Optional: steps-based control and validation split
# Set max_steps to a positive number to override epochs; keep -1 to use epochs
max_steps: -1
val_split: 0.1

# Data sources (choose one)
# 1) Local JSONL dataset (default)
dataset: sample_dataset.jsonl
# 2) Parquet path produced by datasets_logic/preprocessing scripts
parquet_path: data/coedit_parquet/coedit_train.parquet
# 3) Hugging Face dataset id and split (e.g., "grammarly/coedit")
hf_dataset: null
hf_split: train
# Optional filter for CoEdIT-like datasets: only keep rows with matching task (e.g., "gec")
coedit_task: null

# Runtime
device_map: auto  # e.g., "auto", "cpu"
quantization: bnb_8bit  # one of: auto, bnb_4bit, bnb_8bit, none
resume_from_checkpoint: null
debug_tiny: false
bf16: false
attn_implementation: flash_attention_3  # e.g., null, eager, sdpa, flash_attention_2, flash_attention_3

# Logging / evaluation / saving
log_level: INFO
logging_steps: 10
evaluation_strategy: steps
eval_steps: 100
save_steps: 100
save_total_limit: 2
report_to: []  # e.g., ["wandb"]

# Training target formatting
# One of: good_only (default) or structured (Span/Suggestion/Rationale block)
train_target_format: good_only

# Training direction: 'fixer' (bad->good) or 'degrader' (good->bad)
train_direction: degrader

prompt_template: |
  We are creating a dataset for writing text fixer, and you are 
  {context_block}
  Bad:
  {bad}

  Respond EXACTLY with:
  Span: [START:<token_idx>][END:<token_idx>]
  Suggestion: <improved sentence or snippet>
  Rationale: <one-sentence explanation>
